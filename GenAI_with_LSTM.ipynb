{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1EY8h/Wy83fvjeBggde5g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sivakumar793/Gen-AI-with-LSTM---Text-Generation/blob/main/GenAI_with_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import requests\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "XDV3dHO3wID7"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping"
      ],
      "metadata": {
        "id": "3QuBnqnRwIm0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "nvyzIbWewLWl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 0) Settings (tweak here)\n",
        "# ---------------------------\n",
        "TXT_PATH = \"shakespeare_complete_works.txt\"\n",
        "CHECKPOINT_PATH = \"best_lstm_shakespeare.h5\"\n",
        "WORD_TO_ID_PATH = \"word_to_id.json\"\n",
        "ID_TO_WORD_PATH = \"id_to_word.json\""
      ],
      "metadata": {
        "id": "RPU022KIwQ2f"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Memory-friendly defaults (adjust if you have lots of RAM/GPU)\n",
        "VOCAB_SIZE = 5000       # most frequent tokens to keep\n",
        "SEQ_LENGTH = 30         # input words per sequence\n",
        "STEP = 3                # sliding window step (larger -> fewer sequences)\n",
        "EMBEDDING_DIM = 128\n",
        "LSTM_UNITS = 128\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 5\n",
        "PATIENCE = 3            # early stopping patience\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "tf.random.set_seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)"
      ],
      "metadata": {
        "id": "4pmAEWzowX9b"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Download dataset (if missing)\n",
        "# ---------------------------\n",
        "if not os.path.exists(TXT_PATH):\n",
        "    print(\"ğŸ“¥ Downloading Shakespeare (Project Gutenberg)...\")\n",
        "    url = \"https://www.gutenberg.org/cache/epub/100/pg100.txt\"\n",
        "    r = requests.get(url, timeout=30)\n",
        "    r.encoding = \"utf-8\"\n",
        "    with open(TXT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(r.text)\n",
        "    print(\"Saved to\", TXT_PATH)\n",
        "else:\n",
        "    print(\"Found local\", TXT_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcmfnvPYwdHy",
        "outputId": "cd51de33-6ffb-4be1-e8f3-2628937a1073"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found local shakespeare_complete_works.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) Load & preprocess\n",
        "# ---------------------------\n",
        "with open(TXT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Lowercase and strip Gutenberg header/footer if present\n",
        "text = text.lower()\n",
        "start_marker = \"*** start of this project gutenberg ebook\"\n",
        "end_marker = \"*** end of this project gutenberg ebook\"\n",
        "si = text.find(start_marker)\n",
        "if si != -1:\n",
        "    text = text[si + len(start_marker):]\n",
        "ei = text.find(end_marker)\n",
        "if ei != -1:\n",
        "    text = text[:ei]\n",
        "\n",
        "# Remove control chars and many punctuation we don't want\n",
        "text = re.sub(r'[\\x00-\\x1f\\x7f-\\x9f]', ' ', text)\n",
        "text = re.sub(r'[_\\*\\[\\]\\(\\)\\{\\}<>\\/\\\\\"=+#@\\$%&\\^~\\|`]', ' ', text)"
      ],
      "metadata": {
        "id": "yBUW2ntOwiFy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) Tokenize\n",
        "# ---------------------------\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "# keep tokens with letters or digits or apostrophes (filter noisy punctuation)\n",
        "tokens = [t for t in tokens if re.search(r\"[a-z0-9']\", t)]\n",
        "\n",
        "print(\"Total tokens:\", len(tokens))\n",
        "print(\"Example tokens:\", tokens[:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgQv6zoiwqYm",
        "outputId": "43596831-769f-45b6-eb6b-86f48f5d8c9b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total tokens: 988070\n",
            "Example tokens: ['\\ufeffthe', 'project', 'gutenberg', 'ebook', 'of', 'the', 'complete', 'works', 'of', 'william', 'shakespeare', 'this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'united', 'states', 'and', 'most', 'other', 'parts', 'of', 'the']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) Vocabulary (limit vocab)\n",
        "# ---------------------------\n",
        "counter = Counter(tokens)\n",
        "most_common = counter.most_common(VOCAB_SIZE)\n",
        "vocab = [w for w, _ in most_common]\n",
        "\n",
        "# reserve:\n",
        "#   0 -> padding (if needed)\n",
        "#   1..len(vocab) -> vocab tokens\n",
        "#   unknown_token_id = len(vocab) + 1\n",
        "word_to_id = {w: i + 1 for i, w in enumerate(vocab)}\n",
        "unknown_token_id = len(vocab) + 1\n",
        "id_to_word = {str(i + 1): w for i, w in enumerate(vocab)}  # keys as strings for JSON\n",
        "\n",
        "print(\"Vocabulary kept:\", len(vocab))\n",
        "print(\"Unknown token id:\", unknown_token_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHIdvZs_wtvW",
        "outputId": "c07cd6e3-1533-46a0-b1ad-b13313f04770"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary kept: 5000\n",
            "Unknown token id: 5001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5) Encode tokens to ids\n",
        "# ---------------------------\n",
        "token_ids = [word_to_id.get(t, unknown_token_id) for t in tokens]"
      ],
      "metadata": {
        "id": "8AvgfNmWwxHK"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6) Create input-output sequences (memory-friendly sampling)\n",
        "# ---------------------------\n",
        "sequences = []\n",
        "next_words = []\n",
        "for i in range(0, len(token_ids) - SEQ_LENGTH, STEP):\n",
        "    seq = token_ids[i:i + SEQ_LENGTH]\n",
        "    nxt = token_ids[i + SEQ_LENGTH]\n",
        "    sequences.append(seq)\n",
        "    next_words.append(nxt)\n",
        "\n",
        "print(\"Total sequences:\", len(sequences))\n",
        "\n",
        "# Convert to numpy arrays (integers, not one-hot)\n",
        "X = np.array(sequences, dtype=np.int32)\n",
        "y = np.array(next_words, dtype=np.int32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LgDsuT-w07G",
        "outputId": "c01a1f51-7f55-48f4-b722-0e457920a223"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total sequences: 329347\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7) Train / validation split\n",
        "# ---------------------------\n",
        "split_at = int(0.8 * len(X))\n",
        "X_train, X_val = X[:split_at], X[split_at:]\n",
        "y_train, y_val = y[:split_at], y[split_at:]\n",
        "\n",
        "print(\"Train sequences:\", X_train.shape, \"Validation sequences:\", X_val.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twOF4g9kw3QZ",
        "outputId": "97db91b5-9f8e-4e88-fd3d-c84163956ea8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train sequences: (263477, 30) Validation sequences: (65870, 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8) Build model (sparse labels)\n",
        "# ---------------------------\n",
        "num_classes = unknown_token_id + 1  # include unknown and optionally 0 pad\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=num_classes, output_dim=EMBEDDING_DIM, input_length=SEQ_LENGTH),\n",
        "    LSTM(LSTM_UNITS, return_sequences=False),\n",
        "    Dropout(0.2),\n",
        "    Dense(num_classes, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\",  # use sparse labels (integers)\n",
        "    optimizer=\"adam\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "GfRMS6F8w6EQ",
        "outputId": "9505ecf6-2437-4b92-8ae6-61ef266fb478"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)               â”‚ ?                      â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               â”‚ ?                      â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9) Callbacks\n",
        "# ---------------------------\n",
        "checkpoint = ModelCheckpoint(CHECKPOINT_PATH, monitor=\"val_loss\", save_best_only=True, verbose=1)\n",
        "earlystop = EarlyStopping(monitor=\"val_loss\", patience=PATIENCE, restore_best_weights=True, verbose=1)\n"
      ],
      "metadata": {
        "id": "Y1CAVT-ww9de"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10) Option A: Use tf.data pipeline (recommended for larger datasets)\n",
        "# ---------------------------\n",
        "# Convert to tf.data.Dataset for efficient batching/prefetching\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "train_ds = train_ds.shuffle(buffer_size=10000, seed=RANDOM_SEED).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
        "val_ds = val_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "WGaJUxjsxBs7"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 11) Train\n",
        "# ---------------------------\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[checkpoint, earlystop],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"Training finished. Best weights (if improved) saved to:\", CHECKPOINT_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HtOW1ODxEkX",
        "outputId": "98b5fa48-48f9-4f76-d61b-a4dd97901ba1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m4116/4117\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.0706 - loss: 6.2265\n",
            "Epoch 1: val_loss improved from inf to 6.02256, saving model to best_lstm_shakespeare.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m4117/4117\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 63ms/step - accuracy: 0.0706 - loss: 6.2264 - val_accuracy: 0.0983 - val_loss: 6.0226\n",
            "Epoch 2/5\n",
            "\u001b[1m4116/4117\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.1038 - loss: 5.5967\n",
            "Epoch 2: val_loss improved from 6.02256 to 5.91325, saving model to best_lstm_shakespeare.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m4117/4117\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 64ms/step - accuracy: 0.1038 - loss: 5.5967 - val_accuracy: 0.1115 - val_loss: 5.9132\n",
            "Epoch 3/5\n",
            "\u001b[1m4117/4117\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.1203 - loss: 5.3596\n",
            "Epoch 3: val_loss improved from 5.91325 to 5.90397, saving model to best_lstm_shakespeare.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m4117/4117\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 64ms/step - accuracy: 0.1203 - loss: 5.3596 - val_accuracy: 0.1182 - val_loss: 5.9040\n",
            "Epoch 4/5\n",
            "\u001b[1m4117/4117\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.1294 - loss: 5.1964\n",
            "Epoch 4: val_loss improved from 5.90397 to 5.90269, saving model to best_lstm_shakespeare.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m4117/4117\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 64ms/step - accuracy: 0.1294 - loss: 5.1964 - val_accuracy: 0.1205 - val_loss: 5.9027\n",
            "Epoch 5/5\n",
            "\u001b[1m4116/4117\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.1366 - loss: 5.0641\n",
            "Epoch 5: val_loss did not improve from 5.90269\n",
            "\u001b[1m4117/4117\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 64ms/step - accuracy: 0.1366 - loss: 5.0640 - val_accuracy: 0.1233 - val_loss: 5.9119\n",
            "Restoring model weights from the end of the best epoch: 4.\n",
            "Training finished. Best weights (if improved) saved to: best_lstm_shakespeare.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 12) Text generation utilities\n",
        "# ---------------------------\n",
        "\n",
        "def id_to_word_safe(idx):\n",
        "    \"\"\"\n",
        "    idx: integer index\n",
        "    returns readable token string\n",
        "    \"\"\"\n",
        "    if idx == 0:\n",
        "        return \"\"\n",
        "    if str(idx) in id_to_word:\n",
        "        return id_to_word[str(idx)]\n",
        "    if idx == unknown_token_id:\n",
        "        return \"<UNK>\"\n",
        "    return \"<PAD>\""
      ],
      "metadata": {
        "id": "vwMSlXWoxHQy"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_with_temperature(preds, temperature=1.0):\n",
        "    \"\"\"\n",
        "    preds: 1D numpy array of probabilities (model output)\n",
        "    temperature: float - higher -> more random\n",
        "    returns index (int) sampled from distribution\n",
        "    \"\"\"\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    # prevent log(0)\n",
        "    preds = np.clip(preds, 1e-20, 1.0)\n",
        "    log_preds = np.log(preds) / (temperature if temperature > 0 else 1e-9)\n",
        "    exp_preds = np.exp(log_preds)\n",
        "    probs = exp_preds / np.sum(exp_preds)\n",
        "    return np.random.choice(len(probs), p=probs)"
      ],
      "metadata": {
        "id": "CY9KrOIJ1LP_"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, seed_text, length=100, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Generate `length` words given seed_text.\n",
        "    \"\"\"\n",
        "    model_input = []\n",
        "    seed_tokens = word_tokenize(seed_text.lower())\n",
        "    seed_ids = [word_to_id.get(t, unknown_token_id) for t in seed_tokens]\n",
        "\n",
        "    # left-pad with zeros if seed shorter than SEQ_LENGTH\n",
        "    if len(seed_ids) < SEQ_LENGTH:\n",
        "        seed_ids = [0] * (SEQ_LENGTH - len(seed_ids)) + seed_ids\n",
        "    else:  # truncate to last SEQ_LENGTH\n",
        "        seed_ids = seed_ids[-SEQ_LENGTH:]\n",
        "\n",
        "    generated = seed_tokens[:]  # keep human-readable tokens\n",
        "\n",
        "    for _ in range(length):\n",
        "        x = np.array(seed_ids, dtype=np.int32).reshape(1, -1)\n",
        "        preds = model.predict(x, verbose=0)[0]  # shape -> (num_classes,)\n",
        "\n",
        "        next_id = sample_with_temperature(preds, temperature=temperature)\n",
        "        next_word = id_to_word_safe(next_id)\n",
        "        generated.append(next_word)\n",
        "\n",
        "        # slide window\n",
        "        seed_ids = seed_ids[1:] + [next_id]\n",
        "\n",
        "    # join carefully: remove empty tokens\n",
        "    return \" \".join([w for w in generated if w])\n"
      ],
      "metadata": {
        "id": "87VwzdDq2uR4"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If checkpoint exists, load best weights\n",
        "if os.path.exists(CHECKPOINT_PATH):\n",
        "    try:\n",
        "        model.load_weights(CHECKPOINT_PATH)\n",
        "        print(\"Loaded weights from\", CHECKPOINT_PATH)\n",
        "    except Exception as e:\n",
        "        print(\"Could not load checkpoint:\", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvX8Vz-u21Lp",
        "outputId": "efe08d7c-1b80-44b2-acfd-e30b24bf2688"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded weights from best_lstm_shakespeare.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 13) Example generations\n",
        "# ---------------------------\n",
        "seeds = [\n",
        "    \"the king said\",\n",
        "    \"to be or not to be\",\n",
        "    \"in the play\",\n",
        "    \"o my lord\"\n",
        "]\n",
        "\n",
        "for seed in seeds:\n",
        "    print(\"\\nSEED:\", seed)\n",
        "    sample = generate_text(model, seed, length=80, temperature=0.8)\n",
        "    print(sample)\n",
        "    print(\"-\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7QVJFbz24Hr",
        "outputId": "6c990ee6-e4c6-47d9-b759-899c6e4c328b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "SEED: the king said\n",
            "the king said not <UNK> capulet o my lord and <UNK> no greater and <UNK> against the world i shall make it his creature i ll make me juliet i ll tell a piece of my <UNK> <UNK> tranio my lord sir my lord is and <UNK> for your daughter s tears and <UNK> them <UNK> <UNK> too <UNK> and do i ll go to prison petruchio a fashion i did in <UNK> sebastian i that labour will have show a paper and\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "SEED: to be or not to be\n",
            "to be or not to be <UNK> d to the moon to send on hot hours and lusty <UNK> though they may see me and to the <UNK> of your <UNK> and make our company and all the <UNK> degree on <UNK> <UNK> and <UNK> had <UNK> <UNK> in the holy shore which <UNK> the <UNK> so now and at <UNK> of them therefore will <UNK> <UNK> of thee a spirit and fury d and all <UNK> not to can <UNK> my fortunes again i know\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "SEED: in the play\n",
            "in the play of earth second citizen i think you have i think thou that most news romeo i can not pick to thee i saw more than now tis more than to a <UNK> <UNK> at my daughter thy <UNK> <UNK> opinions from the earth ariel now the duke is the great fiend as yet i had not tell thy prayers made thee if i ll be but <UNK> as twere as hot as they are so <UNK> a good <UNK> and\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "SEED: o my lord\n",
            "o my lord lychorida <UNK> <UNK> be the <UNK> will <UNK> <UNK> <UNK> in his <UNK> that you can <UNK> them from the <UNK> <UNK> and <UNK> <UNK> <UNK> <UNK> <UNK> let me play their <UNK> <UNK> from this <UNK> whereof we must met to her and swear the jewel of a <UNK> and and <UNK> as i am <UNK> to true and i was dead there but <UNK> as you were not what i have <UNK> d d that i will have\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 14) Save vocab mappings (JSON-friendly)\n",
        "# ---------------------------\n",
        "# word_to_id: keys are words (strings) -> values are ints\n",
        "with open(WORD_TO_ID_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(word_to_id, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# id_to_word keys are strings already (stringified ids) -> values are words\n",
        "with open(ID_TO_WORD_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(id_to_word, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"Saved vocab mappings to\", WORD_TO_ID_PATH, \"and\", ID_TO_WORD_PATH)\n",
        "print(\"Done.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3jnbeJZ267B",
        "outputId": "de5fcc89-c809-48e7-8796-61f49f78a64a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved vocab mappings to word_to_id.json and id_to_word.json\n",
            "Done.\n"
          ]
        }
      ]
    }
  ]
}